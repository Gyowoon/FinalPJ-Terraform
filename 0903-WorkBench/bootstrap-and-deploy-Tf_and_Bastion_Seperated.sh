#!/bin/bash

# One‑stop automation script for provisioning AWS resources with Terraform
# and deploying the shopping‑mall application to an EKS cluster via a
# bastion host.  This script assumes you have valid AWS credentials
# configured on your local machine (via environment variables or the
# default AWS config files) and that you have a working SSH key for
# connecting to the bastion instance.  The bastion instance will
# perform container image builds and pushes, then apply the Kubernetes
# manifests to the newly created cluster.

set -euo pipefail

# --------------------------------------------------------------------
# Configuration
#
# The following environment variables can be overridden to customise
# the behaviour of this script.  You can export them in your shell or
# define them in an env.sh file which will be sourced automatically
# when present.
#
# TF_DIR          – path to the Terraform directory (default: ./terraform)
# APP_DIR         – path to the application source directory containing
#                    the k8s manifests and Docker build contexts
# REGION          – AWS region (default: ap-northeast-2)
# IMAGE_TAG       – tag to apply to the built Docker images (default: latest)
# SSH_KEY         – path to the private key for SSH into the bastion
# SSH_USER        – user name to use for SSH (default: ec2-user)
# USE_ECR_SECRET  – set to "true" to create a Kubernetes image pull
#                    secret and attach it to the service account
#
# DB_MASTER_USERNAME, DB_MASTER_PASSWORD, DB_NAME – credentials for
# the RDS database.  These must match the values used in your
# terraform.tfvars.  They are used to build the SQLAlchemy URI for
# the backend service.
#
# JWT_SECRET_KEY  – optional pre‑seeded JWT secret.  If omitted a
# random value will be generated by the remote script.
#
# FQDN            – fully qualified domain name for the ingress host.
#
# You can set these variables in an env.sh file in the same directory
# as this script.  That file is ignored by git so you can store
# secrets there safely.  See env.sh.example for a starting point.
# --------------------------------------------------------------------

# Source user‑defined environment variables if present
if [[ -f "$(dirname "$0")/env.sh" ]]; then
  # shellcheck disable=SC1090
  source "$(dirname "$0")/env.sh"
fi

# Apply defaults
TF_DIR=${TF_DIR:-"./"}
APP_DIR=${APP_DIR:-"./"}
REGION=${REGION:-"ap-northeast-2"}
IMAGE_TAG=${IMAGE_TAG:-"latest"}
SSH_USER=${SSH_USER:-"ec2-user"}
USE_ECR_SECRET=${USE_ECR_SECRET:-"false"}

# Resolve required variables
if [[ -z "${SSH_KEY:-}" ]]; then
  echo "ERROR: SSH_KEY environment variable must be set to the path of your bastion SSH private key" >&2
  exit 1
fi
if [[ -z "${DB_MASTER_USERNAME:-}" || -z "${DB_MASTER_PASSWORD:-}" || -z "${DB_NAME:-}" ]]; then
  echo "ERROR: DB_MASTER_USERNAME, DB_MASTER_PASSWORD and DB_NAME must be set to match your terraform.tfvars" >&2
  exit 1
fi

# Ensure the SSH key exists
if [[ ! -f "$SSH_KEY" ]]; then
  echo "ERROR: SSH key file $SSH_KEY not found" >&2
  exit 1
fi

# Convenience function to print headings
heading() {
  echo
  echo "==================== $1 ====================="
}

# 1. Initialise and apply Terraform configuration
## 테라폼 실행부분 
heading "Running terraform apply"

terraform -chdir="$TF_DIR" init -upgrade
terraform -chdir="$TF_DIR" apply -auto-approve

# 2. Retrieve outputs we need
## 테라폼 실행 결과를 변수화 및 터미널로 출력 
heading "Collecting terraform outputs"
CLUSTER_NAME=$(terraform -chdir="$TF_DIR" output -raw cluster_name)
BASTION_IP=$(terraform -chdir="$TF_DIR" output -raw bastion_public_ip)
RDS_ENDPOINT=$(terraform -chdir="$TF_DIR" output -raw rds_endpoint)
REDIS_ENDPOINT=$(terraform -chdir="$TF_DIR" output -raw redis_endpoint)

echo "Cluster name     : $CLUSTER_NAME"
echo "Bastion public IP: $BASTION_IP"
echo "RDS endpoint     : $RDS_ENDPOINT"
echo "Redis endpoint   : $REDIS_ENDPOINT"

# 3. Determine AWS account ID for ECR registry construction
ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
ECR_REGISTRY="${ACCOUNT_ID}.dkr.ecr.${REGION}.amazonaws.com"
echo "AWS Account ID   : $ACCOUNT_ID"

# 4. Copy application artiafacts and prepare remote script
## rsync를 이용해서 Bastion에 전달할, 배포가능한 빌드 결과물/산출물/k8s Manifest(YAMLS)들을 의미함
heading "Uploading application artefacts to bastion"

# The application directory is expected to contain unpacked folders named
# k8s, backend and frontend.  If these folders are missing we abort.
for d in k8s backend frontend; do
  if [[ ! -d "$APP_DIR/$d" ]]; then
    echo "ERROR: directory $APP_DIR/$d not found.  Please ensure your app source directory contains the uncompressed folders." >&2
    exit 1
  fi
done

# Copy the unpacked application folders to the bastion host.  We use rsync
# with --delete to ensure the remote directories exactly mirror the local
# ones.  The trailing slash on the source ensures that rsync copies the
# contents into the destination directory rather than creating a nested
# folder.
## Send some of mainfests(YAML) and RAW F/E & B/E Files from Local to Bastion 
rsync -avz --delete -e "ssh -i $SSH_KEY" "$APP_DIR/backend/" "${SSH_USER}@${BASTION_IP}:~/backend/" >/dev/null
rsync -avz --delete -e "ssh -i $SSH_KEY" "$APP_DIR/frontend/" "${SSH_USER}@${BASTION_IP}:~/frontend/" >/dev/null
rsync -avz --delete -e "ssh -i $SSH_KEY" "$APP_DIR/k8s/" "${SSH_USER}@${BASTION_IP}:~/k8s/" >/dev/null

# Build the remote deployment script in a temporary location.  This
# script contains all the commands needed to build/push images and
# deploy resources into the EKS cluster.  It will be uploaded to
# the bastion but not executed automatically.  You can SSH into the
# bastion and run it manually (e.g. `bash ~/remote_deploy.sh`) after
# reviewing or customizing it.
cat > /tmp/remote_deploy.sh <<'REMOTE_EOF'
#!/bin/bash
set -euo pipefail

# This script runs on the bastion host.  It uses the bastion's IAM role
# to authenticate against AWS services (ECR/EKS) so ensure the instance
# profile attached to the bastion has sufficient permissions.

# Required environment variables passed from the caller:
#   REGION, CLUSTER_NAME, ACCOUNT_ID, IMAGE_TAG, ECR_REGISTRY
#   RDS_ENDPOINT, REDIS_ENDPOINT, DB_MASTER_USERNAME, DB_MASTER_PASSWORD, DB_NAME
#   USE_ECR_SECRET, JWT_SECRET_KEY (optional)

# Use defaults if variables are empty
REGION=${REGION:-"ap-northeast-2"}
IMAGE_TAG=${IMAGE_TAG:-"latest"}
ECR_REGISTRY=${ECR_REGISTRY:-""}
CLUSTER_NAME=${CLUSTER_NAME:-""}
RDS_ENDPOINT=${RDS_ENDPOINT:-""}
REDIS_ENDPOINT=${REDIS_ENDPOINT:-""}
DB_MASTER_USERNAME=${DB_MASTER_USERNAME:-""}
DB_MASTER_PASSWORD=${DB_MASTER_PASSWORD:-""}
DB_NAME=${DB_NAME:-""}
USE_ECR_SECRET=${USE_ECR_SECRET:-"false"}
JWT_SECRET_KEY=${JWT_SECRET_KEY:-""}

if [[ -z "$CLUSTER_NAME" ]]; then
  echo "Cluster name must be provided" >&2; exit 1
fi
if [[ -z "$ECR_REGISTRY" ]]; then
  echo "ECR registry must be provided" >&2; exit 1
fi
if [[ -z "$DB_MASTER_USERNAME" || -z "$DB_MASTER_PASSWORD" || -z "$DB_NAME" ]]; then
  echo "Database credentials must be provided" >&2; exit 1
fi

echo "[Remote] Using cluster $CLUSTER_NAME in region $REGION"
echo "[Remote] ECR registry is $ECR_REGISTRY"

# 1. Authenticate to ECR and create repositories if needed
aws ecr get-login-password --region "$REGION" | sudo docker login --username AWS --password-stdin "$ECR_REGISTRY"

if ! aws ecr describe-repositories --repository-names shop-backend --region "$REGION" >/dev/null 2>&1; then
  aws ecr create-repository --repository-name shop-backend --region "$REGION"
fi
if ! aws ecr describe-repositories --repository-names shop-frontend --region "$REGION" >/dev/null 2>&1; then
  aws ecr create-repository --repository-name shop-frontend --region "$REGION"
fi

# 2. Build and push backend image
echo "[Remote] Building backend image"
cd backend
sudo docker build -t shop-backend:"$IMAGE_TAG" \
  -t "$ECR_REGISTRY/shop-backend:$IMAGE_TAG" \
  -t "$ECR_REGISTRY/shop-backend:latest" .
sudo docker push "$ECR_REGISTRY/shop-backend:$IMAGE_TAG"
sudo docker push "$ECR_REGISTRY/shop-backend:latest"
cd ..

# 3. Build and push frontend image
echo "[Remote] Building frontend image"
cd frontend
sudo docker build -t shop-frontend:"$IMAGE_TAG" \
  -t "$ECR_REGISTRY/shop-frontend:$IMAGE_TAG" \
  -t "$ECR_REGISTRY/shop-frontend:latest" .
sudo docker push "$ECR_REGISTRY/shop-frontend:$IMAGE_TAG"
sudo docker push "$ECR_REGISTRY/shop-frontend:latest"
cd ..

# 4. Update kubeconfig using the bastion's IAM role.  The bastion
# instance profile must have permission to call eks:DescribeCluster and
# eks:GetToken, and its role must be mapped into the cluster RBAC via
# an access entry or the aws‑auth ConfigMap.
aws eks update-kubeconfig --region "$REGION" --name "$CLUSTER_NAME" --alias "$CLUSTER_NAME"

# 5. Install Argo CD CLI and deploy Argo CD via Helm.  The bastion
# already has Helm installed (see user_data in the Terraform for
# aws_instance.bastion).  We first install the argocd CLI for
# convenience, then use Helm to deploy the Argo CD chart into the
# cluster.  The upgrade --install command makes this idempotent so
# re‑running the script does not cause failures.
echo "[Remote] Installing Argo CD CLI and Helm chart"

# Install argocd CLI if not already present
if ! command -v argocd >/dev/null 2>&1; then
  sudo curl -sSL -o /usr/local/bin/argocd \
    https://github.com/argoproj/argo-cd/releases/latest/download/argocd-linux-amd64
  sudo chmod +x /usr/local/bin/argocd
fi

# Ensure the argocd namespace exists
kubectl get ns argocd >/dev/null 2>&1 || kubectl create namespace argocd

# Add the Argo Helm repository and update the repo cache
helm repo add argo https://argoproj.github.io/argo-helm >/dev/null 2>&1 || true
helm repo update >/dev/null 2>&1

# Install or upgrade the argo-cd release.  We set the service type to
# LoadBalancer so that an external endpoint is created automatically.
helm upgrade --install argo-cd argo/argo-cd \
  --namespace argocd \
  --set server.service.type=LoadBalancer

# Wait for the Argo CD server deployment to become available
kubectl -n argocd rollout status deployment/argo-cd-argocd-server

# 6. Ensure the namespace and service account exist
kubectl get ns shop >/dev/null 2>&1 || kubectl create namespace shop
kubectl get serviceaccount shopping-mall-sa -n shop >/dev/null 2>&1 || kubectl create serviceaccount shopping-mall-sa -n shop

# 7. Optionally create imagePullSecret and attach to service account
if [[ "$USE_ECR_SECRET" == "true" ]]; then
  echo "[Remote] Creating image pull secret"
  aws ecr get-login-password --region "$REGION" | kubectl create secret generic ecr-secret \
    --namespace shop --type=kubernetes.io/dockerconfigjson \
    --from-file=.dockerconfigjson=/dev/stdin --dry-run=client -o yaml | kubectl apply -f -
  kubectl patch serviceaccount shopping-mall-sa -n shop -p '{"imagePullSecrets":[{"name":"ecr-secret"}]}'
fi

# 8. Create application secret containing DB and Redis connection strings
DB_URI="mysql+pymysql://${DB_MASTER_USERNAME}:${DB_MASTER_PASSWORD}@${RDS_ENDPOINT}:3306/${DB_NAME}?charset=utf8mb4"
REDIS_URL="redis://${REDIS_ENDPOINT}:6379"

if [[ -z "$JWT_SECRET_KEY" ]]; then
  # Generate a random JWT secret if one was not provided
  JWT_SECRET_KEY=$(openssl rand -hex 32)
fi

kubectl delete secret shop-secrets -n shop >/dev/null 2>&1 || true
kubectl create secret generic shop-secrets -n shop \
  --from-literal=DB_URI="$DB_URI" \
  --from-literal=REDIS_URL="$REDIS_URL" \
  --from-literal=JWT_SECRET_KEY="$JWT_SECRET_KEY"

# 9. Deploy manifests and patch deployments
echo "[Remote] Applying Kubernetes manifests"
# The k8s directory is synced from the local machine.  Apply each manifest
# individually so that partial failures don't stop the script.  The
# namespaces manifest may already exist, so ignore errors for it.
kubectl apply -n shop -f k8s/namespaces.yaml || true
kubectl apply -n shop -f k8s/backend-service.yaml
kubectl apply -n shop -f k8s/frontend-service.yaml
kubectl apply -n shop -f k8s/ingress.yaml
kubectl apply -n shop -f k8s/backend-deployment.yaml
kubectl apply -n shop -f k8s/frontend-deployment.yaml

# Force the service account on the deployments.  If the field already exists
# this patch will replace it.  JSON Patch is used to avoid YAML formatting
# issues.
kubectl -n shop patch deployment backend  --type='json' \
  -p='[{"op":"add","path":"/spec/template/spec/serviceAccountName","value":"shopping-mall-sa"}]'
kubectl -n shop patch deployment frontend --type='json' \
  -p='[{"op":"add","path":"/spec/template/spec/serviceAccountName","value":"shopping-mall-sa"}]'

# Update container images to point at the correct ECR registry and tag
kubectl -n shop set image deployment/backend  backend="$ECR_REGISTRY/shop-backend:$IMAGE_TAG"
kubectl -n shop set image deployment/frontend frontend="$ECR_REGISTRY/shop-frontend:$IMAGE_TAG"

# 10. Wait for the deployments to become available
echo "[Remote] Waiting for rollout to complete"
kubectl -n shop rollout status deployment/backend
kubectl -n shop rollout status deployment/frontend

echo "[Remote] Deployment completed successfully"
REMOTE_EOF

# Upload the remote script and mark it executable so that you can run
# it manually from the bastion if desired.  We do not execute it here.
## Send a script file to Bastion as a reference initial setting 
scp -i "$SSH_KEY" /tmp/remote_deploy.sh "${SSH_USER}@${BASTION_IP}:~/remote_deploy.sh" >/dev/null
ssh -i "$SSH_KEY" "${SSH_USER}@${BASTION_IP}" "chmod +x ~/remote_deploy.sh"

# 5. Write a file containing environment variables to the bastion.  When
# you SSH into the bastion you can source this file to reuse the
# Terraform outputs and other settings.  Note that sensitive values
# like DB credentials are included, so treat this file with care.
## Send some of tfstate Output from Local to Bastion 
heading "Copying environment variables to bastion"
cat > /tmp/bastion_env.sh <<EOF
export REGION=${REGION}
export CLUSTER_NAME=${CLUSTER_NAME}
export ACCOUNT_ID=${ACCOUNT_ID}
export IMAGE_TAG=${IMAGE_TAG}
export ECR_REGISTRY=${ECR_REGISTRY}
export RDS_ENDPOINT=${RDS_ENDPOINT}
export REDIS_ENDPOINT=${REDIS_ENDPOINT}
export DB_MASTER_USERNAME=${DB_MASTER_USERNAME}
export DB_MASTER_PASSWORD=${DB_MASTER_PASSWORD}
export DB_NAME=${DB_NAME}
export USE_ECR_SECRET=${USE_ECR_SECRET}
export JWT_SECRET_KEY=${JWT_SECRET_KEY:-}
EOF
scp -i "$SSH_KEY" /tmp/bastion_env.sh "${SSH_USER}@${BASTION_IP}:~/env.sh" >/dev/null
ssh -i "$SSH_KEY" "${SSH_USER}@${BASTION_IP}" "chmod 600 ~/env.sh"

## Bootstrap Finish Notice 
heading "Bootstrap completed"
echo "Terraform apply and file transfers are complete.  Please SSH into the bastion host (${BASTION_IP}) and source ~/env.sh to load the environment variables.  You can then run ~/remote_deploy.sh manually or execute commands interactively to deploy your application."
